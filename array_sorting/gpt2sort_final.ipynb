{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvemklzjhY1sGE1n6w/X8s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claire-fang/cs182-final-project/blob/main/array_sorting/gpt2sort_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thAp6N3r9sLC",
        "outputId": "ca80cdf2-7c44-4dc3-cf7c-d7c3088e8ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/gpt2_sort\n"
          ]
        }
      ],
      "source": [
        "# @title Mount your Google Drive\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "DRIVE_PATH = '/content/gdrive/My\\ Drive/gpt2_sort'\n",
        "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
        "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "  %mkdir $DRIVE_PATH\n",
        "\n",
        "%cd $DRIVE_PATH\n",
        "\n",
        "## The space in `My Drive` causes some issues, so make a symlink to avoid this.\n",
        "SYM_PATH = '/content/gpt2_sort'\n",
        "if not os.path.exists(SYM_PATH):\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    GPT2Tokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    GPT2Model\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "yaN3fVgX-SjM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 Sort Fine-tuning & Inference with Masked Loss\n",
        "This notebook fine-tunes GPT-2 on a custom sort dataset, masks the prompt tokens in loss so the model only learns to predict the output list, and supports inference."
      ],
      "metadata": {
        "id": "2jx_7KrX-MC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customized Dataset"
      ],
      "metadata": {
        "id": "9KXj3wT8-i14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SortDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each example has a prompt and an answer; inject an EOS after the prompt and another after the answer.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path, tokenizer, max_length=128):\n",
        "        with open(file_path, 'r') as f:\n",
        "            text = f.read().strip()\n",
        "        self.examples = text.split('\\n\\n')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Split into prompt and answer\n",
        "        raw = self.examples[idx]             # e.g. \"Input: [13, 4]\\nOutput: [4, 13]\"\n",
        "        prompt, answer = raw.split('\\n', 1)\n",
        "        eos = self.tokenizer.eos_token\n",
        "\n",
        "        # Inject EOS between prompt and answer, and at the end\n",
        "        full = prompt + eos + answer + eos\n",
        "\n",
        "        tok = self.tokenizer(\n",
        "            full,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids':      tok['input_ids'].squeeze(0),\n",
        "            'attention_mask': tok['attention_mask'].squeeze(0)\n",
        "        }"
      ],
      "metadata": {
        "id": "gxqPRKUS-EGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_collate(batch):\n",
        "    # 1) pull out lists of individual tensors\n",
        "    input_ids_list     = [item['input_ids']     for item in batch]\n",
        "    attention_mask_list= [item['attention_mask'] for item in batch]\n",
        "\n",
        "    # 2) pad up to the longest in this batch\n",
        "    input_ids     = pad_sequence(input_ids_list,     batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_mask= pad_sequence(attention_mask_list,batch_first=True, padding_value=0)\n",
        "\n",
        "    # 3) clone for labels\n",
        "    labels = input_ids.clone()\n",
        "\n",
        "    # 4) mask out the prompt (everything up to and including the first EOS)\n",
        "    eos_id = tokenizer.eos_token_id\n",
        "    for i, seq in enumerate(input_ids):\n",
        "        eos_positions = (seq == eos_id).nonzero(as_tuple=True)[0]\n",
        "        if len(eos_positions)>0:\n",
        "            prompt_end = eos_positions[0].item() + 1\n",
        "        else:\n",
        "            prompt_end = 0\n",
        "        labels[i, :prompt_end] = -100\n",
        "\n",
        "    # 5) mask out any padding positions\n",
        "    labels[attention_mask == 0] = -100\n",
        "\n",
        "    return {\n",
        "        'input_ids':      input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'labels':         labels\n",
        "    }"
      ],
      "metadata": {
        "id": "PDHfODkp-U5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Training Samples"
      ],
      "metadata": {
        "id": "HgfeOAIJ-oK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_random_arrays(num_examples=10000, max_arr_len=5, seed=123, weight=np.array([0.05, 0.05, 0.05, 0.05, 0.8])):\n",
        "  random.seed(seed)\n",
        "  length_num = map(int, weight * num_examples)\n",
        "  examples = []\n",
        "  for len, num in enumerate(length_num):\n",
        "    for _ in range(num):\n",
        "      arr = random.sample(range(100), len + 1)\n",
        "      sorted_arr = sorted(arr)\n",
        "      example = f\"Input: {arr}\\nOutput: {sorted_arr}\"\n",
        "      examples.append(example)\n",
        "  # shuffle the ordering\n",
        "  random.shuffle(examples)\n",
        "  return examples\n",
        "# generate_random_arrays(100)"
      ],
      "metadata": {
        "id": "Aj6qtM6M-e70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # You only need to run this once!\n",
        "# samples = generate_random_arrays()\n",
        "\n",
        "# with open(\"./sort_train_with_weight.txt\", \"w\") as f:\n",
        "#     f.write(\"\\n\\n\".join(samples))"
      ],
      "metadata": {
        "id": "vwOOb2Y_-9IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning GPT2"
      ],
      "metadata": {
        "id": "wi-b-ZhM-sMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_file, model_name='gpt2', output_dir='result',\n",
        "          batch_size=2, epochs=3, max_length=128, save_steps=500):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    global tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # Save tokenizer for inference BEFORE training\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    # dataset\n",
        "    ds = SortDataset(train_file, tokenizer, max_length)\n",
        "    # model\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    model.config.eos_token_id = tokenizer.eos_token_id\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    # training args\n",
        "    args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        overwrite_output_dir=True,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        num_train_epochs=epochs,\n",
        "        save_steps=save_steps,\n",
        "        logging_steps=100,\n",
        "        report_to='none'\n",
        "    )\n",
        "    # trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        data_collator=sort_collate,\n",
        "        train_dataset=ds\n",
        "    )\n",
        "    trainer.train()\n",
        "    trainer.save_model()\n"
      ],
      "metadata": {
        "id": "-0cAzG-u-WsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train('sort_train_with_weight.txt')"
      ],
      "metadata": {
        "id": "0vMf-zmF_Htj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = np.arange(1000, 15001, 1000)\n",
        "losses = [0.245800, 0.074000, 0.054100, 0.029900, 0.026200, 0.019400, 0.016600, 0.013000, 0.011100, 0.006500, 0.003600, 0.003200, 0.001400, 0.001200, 0.000900]\n",
        "plt.plot(steps, losses)"
      ],
      "metadata": {
        "id": "a-e3udPi_Qf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "You can load our fine-tuned model if you don't want to train by *yourself*."
      ],
      "metadata": {
        "id": "zFJufmz7_Va1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(input_sequence: str,\n",
        "          model_dir: str = 'result',\n",
        "          max_new_tokens: int = 20):\n",
        "    # 1) Load tokenizer & model\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.add_special_tokens({'pad_token':'<|pad|>'})\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    model.config.eos_token_id = tokenizer.eos_token_id\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device).eval()\n",
        "\n",
        "    # 2) Tokenize with attention mask\n",
        "    enc = tokenizer(\n",
        "        input_sequence,\n",
        "        return_tensors='pt',\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        add_special_tokens=False\n",
        "    )\n",
        "    input_ids = enc['input_ids'].to(device)\n",
        "    attention_mask = enc['attention_mask'].to(device)\n",
        "\n",
        "    # 3) Generate *only* max_new_tokens\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,    # ← switch to greedy\n",
        "        num_beams=1,        # ← no beam search, just greedy\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "    # 4) Slice off the prompt tokens and decode only the new ones\n",
        "    gen_tokens = outputs[0, input_ids.shape[-1]:]\n",
        "    result = tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
        "    result = result.lstrip()\n",
        "    # print(result)\n",
        "    return result"
      ],
      "metadata": {
        "id": "Khy5soB3_TW9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Length-2 Array Sorting"
      ],
      "metadata": {
        "id": "e8QrEKX6QdD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate length \"len\" arrays\n",
        "def inference_with_len(seed, len, num_samples=100):\n",
        "  num_correct = 0\n",
        "  random.seed(seed)\n",
        "  for i in range(num_samples):\n",
        "    lst = []\n",
        "    input = \"Input: [\"\n",
        "    for j in range(len):\n",
        "      random_a = random.randint(0, 100)\n",
        "      lst.append(random_a)\n",
        "    # sort a, b, c with python library:\n",
        "    sorted_lst = sorted(lst)\n",
        "    input = \"Input: \" + str(lst) + \"\\nOutput:\"\n",
        "    # print(input)\n",
        "    if infer(input) == f'{sorted_lst}':\n",
        "      num_correct += 1\n",
        "    if i % 10 == 0:\n",
        "        print(i)\n",
        "        print(f'Accuracy: {num_correct / (i+1)}')\n",
        "\n",
        "  print(num_samples)\n",
        "  print(f'Accuracy: {num_correct / num_samples}')"
      ],
      "metadata": {
        "id": "oDaaDNXQQ4Ky"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_with_len(12345, 2, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYDrp-gYWUcS",
        "outputId": "c2b4bc13-d61b-4421-b3d8-cced8afaa451"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Accuracy: 1.0\n",
            "10\n",
            "Accuracy: 1.0\n",
            "20\n",
            "Accuracy: 1.0\n",
            "30\n",
            "Accuracy: 1.0\n",
            "40\n",
            "Accuracy: 1.0\n",
            "50\n",
            "Accuracy: 1.0\n",
            "60\n",
            "Accuracy: 1.0\n",
            "70\n",
            "Accuracy: 1.0\n",
            "80\n",
            "Accuracy: 1.0\n",
            "90\n",
            "Accuracy: 1.0\n",
            "100\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Length-3 Array Sorting"
      ],
      "metadata": {
        "id": "u-xhkMp_QktV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_with_len(23456, 3, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbxsKuq6QbMd",
        "outputId": "3d32d6c6-ce53-4fc3-d077-73f04c25ba2c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Accuracy: 1.0\n",
            "10\n",
            "Accuracy: 1.0\n",
            "20\n",
            "Accuracy: 1.0\n",
            "30\n",
            "Accuracy: 1.0\n",
            "40\n",
            "Accuracy: 1.0\n",
            "50\n",
            "Accuracy: 0.9803921568627451\n",
            "60\n",
            "Accuracy: 0.9836065573770492\n",
            "70\n",
            "Accuracy: 0.9859154929577465\n",
            "80\n",
            "Accuracy: 0.9876543209876543\n",
            "90\n",
            "Accuracy: 0.989010989010989\n",
            "100\n",
            "Accuracy: 0.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Length-4 Array Sorting"
      ],
      "metadata": {
        "id": "yUklpTmyQmah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_with_len(34567, 4, 100)"
      ],
      "metadata": {
        "id": "2__5Oe-pQpYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Length-5 Array Sorting"
      ],
      "metadata": {
        "id": "whEUIlFjWfm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_with_len(45678, 5, 100)"
      ],
      "metadata": {
        "id": "6rPOLbeOWexT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Length-6 Array Sorting"
      ],
      "metadata": {
        "id": "CDiUt6LVWl7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_with_len(56789, 6, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLIsFsVuWlcG",
        "outputId": "3f2bfe18-daca-4444-e1ed-d222290fc410"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Accuracy: 1.0\n",
            "10\n",
            "Accuracy: 0.7272727272727273\n",
            "20\n",
            "Accuracy: 0.6190476190476191\n",
            "30\n",
            "Accuracy: 0.5806451612903226\n",
            "40\n",
            "Accuracy: 0.5121951219512195\n",
            "50\n",
            "Accuracy: 0.47058823529411764\n",
            "60\n",
            "Accuracy: 0.45901639344262296\n",
            "70\n",
            "Accuracy: 0.4225352112676056\n",
            "80\n",
            "Accuracy: 0.43209876543209874\n",
            "90\n",
            "Accuracy: 0.4725274725274725\n",
            "100\n",
            "Accuracy: 0.46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference_with_len(56789, 7, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvCaIJENXOf-",
        "outputId": "176c544f-f0be-46bf-e1ed-b0519168dcc2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Accuracy: 0.0\n",
            "10\n",
            "Accuracy: 0.5454545454545454\n",
            "20\n",
            "Accuracy: 0.7142857142857143\n",
            "30\n",
            "Accuracy: 0.6451612903225806\n",
            "40\n",
            "Accuracy: 0.6097560975609756\n",
            "50\n",
            "Accuracy: 0.5882352941176471\n",
            "60\n",
            "Accuracy: 0.5245901639344263\n",
            "70\n",
            "Accuracy: 0.5070422535211268\n",
            "80\n",
            "Accuracy: 0.5308641975308642\n",
            "90\n",
            "Accuracy: 0.5164835164835165\n",
            "100\n",
            "Accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Probing"
      ],
      "metadata": {
        "id": "bFKcrR4OXqgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Fine-tuned result\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"result\")\n",
        "model = GPT2Model.from_pretrained(\"result\", output_hidden_states=True)\n",
        "model.eval()\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Generate synthetic data\n",
        "def generate_example():\n",
        "    arr = random.sample(range(100), 5)\n",
        "    sorted_arr = sorted(arr)\n",
        "    return arr, [sorted_arr.index(x) for x in arr]  # arr, sorted_positions\n",
        "\n",
        "# Get hidden states for number tokens\n",
        "def extract_token_hidden_states(arr, labels):\n",
        "    layer_dict = {}\n",
        "    prompt = f\"Input: {arr}\\nOutput:\"\n",
        "    # Tokenize with attention mask\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs) # perform a forward pass\n",
        "\n",
        "    tokens = tokenizer.tokenize(prompt)\n",
        "    token_ids = inputs['input_ids'][0].tolist()\n",
        "    decoded_tokens = [tokenizer.decode([tok]) for tok in token_ids]\n",
        "\n",
        "    for layer in range(13):\n",
        "        hidden = outputs.hidden_states[layer][0]  # shape: [seq_len, hidden_dim]; use [0] since we only have one batch\n",
        "\n",
        "        features = []\n",
        "        final_labels = []\n",
        "\n",
        "        for i, tok in enumerate(decoded_tokens):\n",
        "            try:\n",
        "                # Only keep number tokens (e.g. \"3\", \"Ġ1\", \"Ġ4\")\n",
        "                val = int(tok.strip())\n",
        "                if val in arr:\n",
        "                    idx_in_arr = arr.index(val)\n",
        "                    features.append(hidden[i].numpy()) # the hidden state correspinding to the ith element inside arr\n",
        "                    final_labels.append(labels[idx_in_arr])\n",
        "            except:\n",
        "                continue\n",
        "        layer_dict[layer] = (features, final_labels)\n",
        "\n",
        "    return layer_dict"
      ],
      "metadata": {
        "id": "KsM714KqXWQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build dataset for each layer\n",
        "samples = []\n",
        "for _ in range(500):  # generate 500 samples\n",
        "  samples.append(generate_example())\n",
        "\n",
        "X_dict, y_dict = defaultdict(list), defaultdict(list)\n",
        "for arr, labels in samples:\n",
        "  layer_dict = extract_token_hidden_states(arr, labels)\n",
        "  for layer in range(13):\n",
        "    feats, labs = layer_dict[layer]\n",
        "    X_dict[layer].extend(feats)\n",
        "    y_dict[layer].extend(labs)"
      ],
      "metadata": {
        "id": "rEB0CPVNZEKU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNreBrlGWZaSbQ9oA5oEcvQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claire-fang/cs182-final-project/blob/main/array_sorting/gpt2sort_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thAp6N3r9sLC",
        "outputId": "ad8c71b5-2e22-43fb-a72a-f0eb6dc0fb61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/gpt2_sort\n"
          ]
        }
      ],
      "source": [
        "# @title Mount your Google Drive\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "DRIVE_PATH = '/content/gdrive/My\\ Drive/gpt2_sort'\n",
        "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
        "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "  %mkdir $DRIVE_PATH\n",
        "\n",
        "%cd $DRIVE_PATH\n",
        "\n",
        "## The space in `My Drive` causes some issues, so make a symlink to avoid this.\n",
        "SYM_PATH = '/content/gpt2_sort'\n",
        "if not os.path.exists(SYM_PATH):\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    GPT2Tokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    GPT2Model\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "yaN3fVgX-SjM"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 Sort Fine-tuning & Inference with Masked Loss\n",
        "This notebook fine-tunes GPT-2 on a custom sort dataset, masks the prompt tokens in loss so the model only learns to predict the output list, and supports inference."
      ],
      "metadata": {
        "id": "2jx_7KrX-MC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customized Dataset"
      ],
      "metadata": {
        "id": "9KXj3wT8-i14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SortDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each example has a prompt and an answer; inject an EOS after the prompt and another after the answer.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path, tokenizer, max_length=128):\n",
        "        with open(file_path, 'r') as f:\n",
        "            text = f.read().strip()\n",
        "        self.examples = text.split('\\n\\n')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Split into prompt and answer\n",
        "        raw = self.examples[idx]             # e.g. \"Input: [13, 4]\\nOutput: [4, 13]\"\n",
        "        prompt, answer = raw.split('\\n', 1)\n",
        "        eos = self.tokenizer.eos_token\n",
        "\n",
        "        # Inject EOS between prompt and answer, and at the end\n",
        "        full = prompt + eos + answer + eos\n",
        "\n",
        "        tok = self.tokenizer(\n",
        "            full,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids':      tok['input_ids'].squeeze(0),\n",
        "            'attention_mask': tok['attention_mask'].squeeze(0)\n",
        "        }"
      ],
      "metadata": {
        "id": "gxqPRKUS-EGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_collate(batch):\n",
        "    # 1) pull out lists of individual tensors\n",
        "    input_ids_list     = [item['input_ids']     for item in batch]\n",
        "    attention_mask_list= [item['attention_mask'] for item in batch]\n",
        "\n",
        "    # 2) pad up to the longest in this batch\n",
        "    input_ids     = pad_sequence(input_ids_list,     batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_mask= pad_sequence(attention_mask_list,batch_first=True, padding_value=0)\n",
        "\n",
        "    # 3) clone for labels\n",
        "    labels = input_ids.clone()\n",
        "\n",
        "    # 4) mask out the prompt (everything up to and including the first EOS)\n",
        "    eos_id = tokenizer.eos_token_id\n",
        "    for i, seq in enumerate(input_ids):\n",
        "        eos_positions = (seq == eos_id).nonzero(as_tuple=True)[0]\n",
        "        if len(eos_positions)>0:\n",
        "            prompt_end = eos_positions[0].item() + 1\n",
        "        else:\n",
        "            prompt_end = 0\n",
        "        labels[i, :prompt_end] = -100\n",
        "\n",
        "    # 5) mask out any padding positions\n",
        "    labels[attention_mask == 0] = -100\n",
        "\n",
        "    return {\n",
        "        'input_ids':      input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'labels':         labels\n",
        "    }"
      ],
      "metadata": {
        "id": "PDHfODkp-U5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Training Samples"
      ],
      "metadata": {
        "id": "HgfeOAIJ-oK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_random_arrays(num_examples=10000, max_arr_len=5, seed=123, weight=np.array([0.05, 0.05, 0.05, 0.05, 0.8])):\n",
        "  random.seed(seed)\n",
        "  length_num = map(int, weight * num_examples)\n",
        "  examples = []\n",
        "  for len, num in enumerate(length_num):\n",
        "    for _ in range(num):\n",
        "      arr = random.sample(range(100), len + 1)\n",
        "      sorted_arr = sorted(arr)\n",
        "      example = f\"Input: {arr}\\nOutput: {sorted_arr}\"\n",
        "      examples.append(example)\n",
        "  # shuffle the ordering\n",
        "  random.shuffle(examples)\n",
        "  return examples\n",
        "# generate_random_arrays(100)"
      ],
      "metadata": {
        "id": "Aj6qtM6M-e70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # You only need to run this once!\n",
        "# samples = generate_random_arrays()\n",
        "\n",
        "# with open(\"./sort_train_with_weight.txt\", \"w\") as f:\n",
        "#     f.write(\"\\n\\n\".join(samples))"
      ],
      "metadata": {
        "id": "vwOOb2Y_-9IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning GPT2"
      ],
      "metadata": {
        "id": "wi-b-ZhM-sMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_file, model_name='gpt2', output_dir='result',\n",
        "          batch_size=2, epochs=3, max_length=128, save_steps=500):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    global tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # Save tokenizer for inference BEFORE training\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    # dataset\n",
        "    ds = SortDataset(train_file, tokenizer, max_length)\n",
        "    # model\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    model.config.eos_token_id = tokenizer.eos_token_id\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    # training args\n",
        "    args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        overwrite_output_dir=True,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        num_train_epochs=epochs,\n",
        "        save_steps=save_steps,\n",
        "        logging_steps=100,\n",
        "        report_to='none'\n",
        "    )\n",
        "    # trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        data_collator=sort_collate,\n",
        "        train_dataset=ds\n",
        "    )\n",
        "    trainer.train()\n",
        "    trainer.save_model()\n"
      ],
      "metadata": {
        "id": "-0cAzG-u-WsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train('sort_train_with_weight.txt')"
      ],
      "metadata": {
        "id": "0vMf-zmF_Htj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = np.arange(1000, 15001, 1000)\n",
        "losses = [0.245800, 0.074000, 0.054100, 0.029900, 0.026200, 0.019400, 0.016600, 0.013000, 0.011100, 0.006500, 0.003600, 0.003200, 0.001400, 0.001200, 0.000900]\n",
        "plt.plot(steps, losses)"
      ],
      "metadata": {
        "id": "a-e3udPi_Qf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "You can load our fine-tuned model if you don't want to train by *yourself*."
      ],
      "metadata": {
        "id": "zFJufmz7_Va1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(input_sequence: str,\n",
        "          model_dir: str = 'result',\n",
        "          max_new_tokens: int = 20):\n",
        "    # 1) Load tokenizer & model\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.add_special_tokens({'pad_token':'<|pad|>'})\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    model.config.eos_token_id = tokenizer.eos_token_id\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device).eval()\n",
        "\n",
        "    # 2) Tokenize with attention mask\n",
        "    enc = tokenizer(\n",
        "        input_sequence,\n",
        "        return_tensors='pt',\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        add_special_tokens=False\n",
        "    )\n",
        "    input_ids = enc['input_ids'].to(device)\n",
        "    attention_mask = enc['attention_mask'].to(device)\n",
        "\n",
        "    # 3) Generate *only* max_new_tokens\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,    # ← switch to greedy\n",
        "        num_beams=1,        # ← no beam search, just greedy\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "    # 4) Slice off the prompt tokens and decode only the new ones\n",
        "    gen_tokens = outputs[0, input_ids.shape[-1]:]\n",
        "    result = tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
        "    result = result.lstrip()\n",
        "    # print(result)\n",
        "    return result"
      ],
      "metadata": {
        "id": "Khy5soB3_TW9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Length-2 Array Sorting"
      ],
      "metadata": {
        "id": "e8QrEKX6QdD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate length \"len\" arrays\n",
        "def inference_with_len(seed, len, num_samples=100):\n",
        "  num_correct = 0\n",
        "  random.seed(seed)\n",
        "  for i in range(num_samples):\n",
        "    lst = []\n",
        "    input = \"Input: [\"\n",
        "    for j in range(len):\n",
        "      random_a = random.randint(0, 100)\n",
        "      lst.append(random_a)\n",
        "    # sort a, b, c with python library:\n",
        "    sorted_lst = sorted(lst)\n",
        "    input = \"Input: \" + str(lst) + \"\\nOutput:\"\n",
        "    # print(input)\n",
        "    if infer(input) == f'{sorted_lst}':\n",
        "      num_correct += 1\n",
        "    if i % 10 == 0:\n",
        "        print(i)\n",
        "        print(f'Accuracy: {num_correct / (i+1)}')\n",
        "\n",
        "  print(num_samples)\n",
        "  print(f'Accuracy: {num_correct / num_samples}')"
      ],
      "metadata": {
        "id": "oDaaDNXQQ4Ky"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_with_len(12345, 2, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYDrp-gYWUcS",
        "outputId": "c2b4bc13-d61b-4421-b3d8-cced8afaa451"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Accuracy: 1.0\n",
            "10\n",
            "Accuracy: 1.0\n",
            "20\n",
            "Accuracy: 1.0\n",
            "30\n",
            "Accuracy: 1.0\n",
            "40\n",
            "Accuracy: 1.0\n",
            "50\n",
            "Accuracy: 1.0\n",
            "60\n",
            "Accuracy: 1.0\n",
            "70\n",
            "Accuracy: 1.0\n",
            "80\n",
            "Accuracy: 1.0\n",
            "90\n",
            "Accuracy: 1.0\n",
            "100\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Length-3 Array Sorting"
      ],
      "metadata": {
        "id": "u-xhkMp_QktV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_with_len(23456, 3, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbxsKuq6QbMd",
        "outputId": "3d32d6c6-ce53-4fc3-d077-73f04c25ba2c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Accuracy: 1.0\n",
            "10\n",
            "Accuracy: 1.0\n",
            "20\n",
            "Accuracy: 1.0\n",
            "30\n",
            "Accuracy: 1.0\n",
            "40\n",
            "Accuracy: 1.0\n",
            "50\n",
            "Accuracy: 0.9803921568627451\n",
            "60\n",
            "Accuracy: 0.9836065573770492\n",
            "70\n",
            "Accuracy: 0.9859154929577465\n",
            "80\n",
            "Accuracy: 0.9876543209876543\n",
            "90\n",
            "Accuracy: 0.989010989010989\n",
            "100\n",
            "Accuracy: 0.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Length-4 Array Sorting"
      ],
      "metadata": {
        "id": "yUklpTmyQmah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_with_len(34567, 4, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2__5Oe-pQpYg",
        "outputId": "ef393139-1aca-45ce-a0b1-b6e82224ac44"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Accuracy: 1.0\n",
            "10\n",
            "Accuracy: 1.0\n",
            "20\n",
            "Accuracy: 1.0\n",
            "30\n",
            "Accuracy: 1.0\n",
            "40\n",
            "Accuracy: 1.0\n",
            "50\n",
            "Accuracy: 0.9803921568627451\n",
            "60\n",
            "Accuracy: 0.9836065573770492\n",
            "70\n",
            "Accuracy: 0.971830985915493\n",
            "80\n",
            "Accuracy: 0.9753086419753086\n",
            "90\n",
            "Accuracy: 0.978021978021978\n",
            "100\n",
            "Accuracy: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Length-5 Array Sorting"
      ],
      "metadata": {
        "id": "whEUIlFjWfm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_with_len(45678, 5, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rPOLbeOWexT",
        "outputId": "266d3235-587d-4ba6-e0c1-b06f50a0f94a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Accuracy: 1.0\n",
            "10\n",
            "Accuracy: 1.0\n",
            "20\n",
            "Accuracy: 1.0\n",
            "30\n",
            "Accuracy: 1.0\n",
            "40\n",
            "Accuracy: 1.0\n",
            "50\n",
            "Accuracy: 0.9803921568627451\n",
            "60\n",
            "Accuracy: 0.9836065573770492\n",
            "70\n",
            "Accuracy: 0.9859154929577465\n",
            "80\n",
            "Accuracy: 0.9753086419753086\n",
            "90\n",
            "Accuracy: 0.9560439560439561\n",
            "100\n",
            "Accuracy: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Length-6 Array Sorting"
      ],
      "metadata": {
        "id": "CDiUt6LVWl7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_with_len(56789, 6, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLIsFsVuWlcG",
        "outputId": "3f2bfe18-daca-4444-e1ed-d222290fc410"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Accuracy: 1.0\n",
            "10\n",
            "Accuracy: 0.7272727272727273\n",
            "20\n",
            "Accuracy: 0.6190476190476191\n",
            "30\n",
            "Accuracy: 0.5806451612903226\n",
            "40\n",
            "Accuracy: 0.5121951219512195\n",
            "50\n",
            "Accuracy: 0.47058823529411764\n",
            "60\n",
            "Accuracy: 0.45901639344262296\n",
            "70\n",
            "Accuracy: 0.4225352112676056\n",
            "80\n",
            "Accuracy: 0.43209876543209874\n",
            "90\n",
            "Accuracy: 0.4725274725274725\n",
            "100\n",
            "Accuracy: 0.46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference_with_len(56789, 7, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvCaIJENXOf-",
        "outputId": "176c544f-f0be-46bf-e1ed-b0519168dcc2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Accuracy: 0.0\n",
            "10\n",
            "Accuracy: 0.5454545454545454\n",
            "20\n",
            "Accuracy: 0.7142857142857143\n",
            "30\n",
            "Accuracy: 0.6451612903225806\n",
            "40\n",
            "Accuracy: 0.6097560975609756\n",
            "50\n",
            "Accuracy: 0.5882352941176471\n",
            "60\n",
            "Accuracy: 0.5245901639344263\n",
            "70\n",
            "Accuracy: 0.5070422535211268\n",
            "80\n",
            "Accuracy: 0.5308641975308642\n",
            "90\n",
            "Accuracy: 0.5164835164835165\n",
            "100\n",
            "Accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Probing"
      ],
      "metadata": {
        "id": "bFKcrR4OXqgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Fine-tuned result\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"result\")\n",
        "model = GPT2Model.from_pretrained(\"result\", output_hidden_states=True)\n",
        "model.eval()\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Generate synthetic data\n",
        "def generate_example():\n",
        "    arr = random.sample(range(100), 5)\n",
        "    sorted_arr = sorted(arr)\n",
        "    return arr, [sorted_arr.index(x) for x in arr]  # arr, sorted_positions\n",
        "\n",
        "# Get hidden states for number tokens\n",
        "def extract_token_hidden_states(arr, labels):\n",
        "    layer_dict = {}\n",
        "    prompt = f\"Input: {arr}\\nOutput:\"\n",
        "    # Tokenize with attention mask\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs) # perform a forward pass\n",
        "\n",
        "    tokens = tokenizer.tokenize(prompt)\n",
        "    token_ids = inputs['input_ids'][0].tolist()\n",
        "    decoded_tokens = [tokenizer.decode([tok]) for tok in token_ids]\n",
        "\n",
        "    for layer in range(13):\n",
        "        hidden = outputs.hidden_states[layer][0]  # shape: [seq_len, hidden_dim]; use [0] since we only have one batch\n",
        "\n",
        "        features = []\n",
        "        final_labels = []\n",
        "\n",
        "        for i, tok in enumerate(decoded_tokens):\n",
        "            try:\n",
        "                # Only keep number tokens (e.g. \"3\", \"Ġ1\", \"Ġ4\")\n",
        "                val = int(tok.strip())\n",
        "                if val in arr:\n",
        "                    idx_in_arr = arr.index(val)\n",
        "                    features.append(hidden[i].numpy()) # the hidden state correspinding to the ith element inside arr\n",
        "                    final_labels.append(labels[idx_in_arr])\n",
        "            except:\n",
        "                continue\n",
        "        layer_dict[layer] = (features, final_labels)\n",
        "\n",
        "    return layer_dict"
      ],
      "metadata": {
        "id": "KsM714KqXWQI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build dataset for each layer\n",
        "samples = []\n",
        "for _ in range(500):  # generate 500 samples\n",
        "  samples.append(generate_example())\n",
        "\n",
        "X_dict, y_dict = defaultdict(list), defaultdict(list)\n",
        "for arr, labels in samples:\n",
        "  layer_dict = extract_token_hidden_states(arr, labels)\n",
        "  for layer in range(13):\n",
        "    feats, labs = layer_dict[layer]\n",
        "    X_dict[layer].extend(feats)\n",
        "    y_dict[layer].extend(labs)"
      ],
      "metadata": {
        "id": "rEB0CPVNZEKU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "accs = []\n",
        "y_pred_dict = {}\n",
        "for layer in range(13):\n",
        "  # Train linear probe\n",
        "  # X_train, X_test, y_train, y_test = train_test_split(X_dict[layer], y_dict[layer], test_size=0.2)\n",
        "  X_train, X_test, y_train, y_test = X_dict[layer][:1600], X_dict[layer][1600:], y_dict[layer][:1600], y_dict[layer][1600:]\n",
        "\n",
        "  # standardization\n",
        "  scaler_train = preprocessing.StandardScaler().fit(X_train)\n",
        "  X_train = scaler_train.transform(X_train)\n",
        "  scaler_test = preprocessing.StandardScaler().fit(X_test)\n",
        "  X_test = scaler_test.transform(X_test)\n",
        "\n",
        "  clf = LogisticRegression(max_iter=1500)\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  # Evaluate\n",
        "  acc = accuracy_score(y_test, y_pred)\n",
        "  y_pred_dict[layer] = y_pred\n",
        "  accs.append(acc)\n",
        "  print(f\"Linear probe accuracy for layer {layer}: {acc:.4f}\")"
      ],
      "metadata": {
        "id": "BsBfckup10X_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x=list(range(13)), y=accs)\n",
        "sns.lineplot(x=list(range(13)), y=accs)\n",
        "plt.xlabel(\"Layer\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy of predicting token position in sorted array using each layer's hidden states\")"
      ],
      "metadata": {
        "id": "93s7CFGO119-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuned Lens"
      ],
      "metadata": {
        "id": "FKmfjHXZr0xQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup Environment for Tuned Lens\n",
        "!git clone https://github.com/AlignmentResearch/tuned-lens.git\n",
        "%cd tuned-lens\n",
        "!pip install -e \".[dev]\"\n",
        "!pre-commit install\n",
        "# 1) Re-pin torchdata to the last version with dataloader2\n",
        "!pip install torchdata==0.8.0 --force-reinstall --no-deps\n",
        "\n",
        "# 2) Install your cloned tuned-lens in editable mode, but don't install its dependencies\n",
        "!pip install --no-deps -e /content/tuned-lens"
      ],
      "metadata": {
        "id": "GrwKZ__Ir2SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate .jsonl file\n",
        "# Generate .jsonl file to satisfy tuned lens format\n",
        "import re, json\n",
        "\n",
        "# 1) tighter bracket-only capture, allow multiline\n",
        "pattern = re.compile(\n",
        "    r\"Input:\\s*\\[([^\\]]*)\\]\\s*Output:\\s*\\[([^\\]]*)\\]\",\n",
        "    re.DOTALL | re.MULTILINE\n",
        ")\n",
        "\n",
        "# 2) load\n",
        "with open(\"../sort_train_with_weight.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# 3) debug print\n",
        "# print(\"First 200 chars of file:\\n\", text[:200])\n",
        "# print(\"Matches found:\", len(pattern.findall(text)))\n",
        "\n",
        "# 4) write JSONL\n",
        "matches = list(pattern.finditer(text))\n",
        "if matches:\n",
        "    with open(\"sort_train_with_weight.jsonl\", \"w\") as fout:\n",
        "        for m in matches:\n",
        "            # inp  = [int(x) for x in m.group(1).split(',') if x.strip()]\n",
        "            # outp = [int(x) for x in m.group(2).split(',') if x.strip()]\n",
        "            # fout.write(json.dumps({\"input\": inp, \"output\": outp}) + \"\\n\")\n",
        "            fout.write(json.dumps({\"text\": m.group(0)}) + \"\\n\")\n",
        "    print(f\"Wrote {len(matches)} records to sort_train_with_weight.jsonl\")\n",
        "else:\n",
        "    print(\"No Input/Output blocks found—nothing written.\")\n"
      ],
      "metadata": {
        "id": "vitWKT2A1_dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Check Torchdata Version\n",
        "# Make sure the version is 0.8.0\n",
        "import torchdata, tuned_lens\n",
        "from torchdata import dataloader2\n",
        "print(\"torchdata:\", torchdata.__version__)\n",
        "print(\"tuned_lens CLI ready!\")"
      ],
      "metadata": {
        "id": "ykd8qCXD2Q4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m tuned_lens train -h"
      ],
      "metadata": {
        "id": "BE9e9WKV2XAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train Tuned Lens\n",
        "# run this section only once; if you don't want to train, you can load our trained version\n",
        "# !python -m tuned_lens train \\\n",
        "#   --model.name ../result \\\n",
        "#   --data.name sort_train_with_weight.jsonl \\\n",
        "#   --text_column text \\\n",
        "#   --per_gpu_batch_size 2 \\\n",
        "#   --tokens_per_step 1024 \\\n",
        "#   --num_steps 25 \\\n",
        "#   --output ../sorted-lens/gpt2-sort\n"
      ],
      "metadata": {
        "id": "lCTuUAck2ZRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load our Finetuned Model and Trained Tuned Lens\n",
        "import torch\n",
        "from tuned_lens.nn.lenses import TunedLens\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# To try a diffrent modle / lens check if the lens is avalible then modify this code\n",
        "model_dir = '../result'\n",
        "model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
        "model = model.to(device)\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n",
        "lens_dir = '../sorted-lens/gpt2-sort'\n",
        "tuned_lens = TunedLens.from_model_and_pretrained(model, lens_dir)\n",
        "tuned_lens = tuned_lens.to(device)"
      ],
      "metadata": {
        "id": "LjAmtGuP2lXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup Experiment samples\n",
        "input_1 = tokenizer.encode(\n",
        "    'Input: [6, 34, 11, 98, 52]\\nOutput: [6, 11, 34, 52, 98]'\n",
        ")\n",
        "input_2 = tokenizer.encode(\n",
        "    'Input: [34, 13, 4, 48, 68]\\nOutput: [4, 13, 34, 48, 68]'\n",
        ")\n",
        "input_3 = tokenizer.encode(\n",
        "    'Input: [71, 42, 43, 6, 20]\\nOutput: [6, 20, 42, 43, 71]'\n",
        ")\n",
        "input_4 = tokenizer.encode(\n",
        "    'Input: [31, 20, 0, 55, 99]\\nOutput: [0, 20, 31, 55, 99]'\n",
        ")\n",
        "input_5 = tokenizer.encode(\n",
        "    'Input: [60, 4, 98, 39, 17]\\nOutput: [4, 17, 39, 60, 98]'\n",
        ")\n",
        "input_6 = tokenizer.encode(\n",
        "    'Input: [16, 12, 92, 33, 80]\\nOutput: [12, 16, 33, 80, 92]'\n",
        ")\n",
        "\n",
        "targets_1 = input_1[1:] + [tokenizer.eos_token_id]\n",
        "targets_2 = input_2[1:] + [tokenizer.eos_token_id]\n",
        "targets_3 = input_3[1:] + [tokenizer.eos_token_id]\n",
        "targets_4 = input_4[1:] + [tokenizer.eos_token_id]\n",
        "targets_5 = input_5[1:] + [tokenizer.eos_token_id]\n",
        "targets_6 = input_6[1:] + [tokenizer.eos_token_id]\n",
        "# targets_model = input_ids_model[1:] + [tokenizer.eos_token_id]\n",
        "\n",
        "print(tokenizer.convert_ids_to_tokens(input_1))\n",
        "# print the length of the tokens\n",
        "print(len(input_1))\n",
        "# print(tokenizer.convert_ids_to_tokens(input_ids_model))"
      ],
      "metadata": {
        "id": "_ooqb5Ge58xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate length-5 arrays\n",
        "def generate_tests(num_examples=10000, seed=123):\n",
        "  random.seed(seed)\n",
        "  examples = []\n",
        "  for _ in range(num_examples):\n",
        "    arr = random.sample(range(100), 5)\n",
        "    sorted_arr = sorted(arr)\n",
        "    example = f\"Input: {arr}\\nOutput: {sorted_arr}\"\n",
        "    examples.append(example)\n",
        "  # shuffle the ordering\n",
        "  random.shuffle(examples)\n",
        "  return examples\n",
        "\n",
        "def percentage_to_float(percentage_string):\n",
        "  try:\n",
        "      clean_string = percentage_string.strip().replace(\",\", \".\")\n",
        "      numeric_value = float(clean_string.replace(\"%\", \"\"))\n",
        "      return numeric_value / 100.0\n",
        "  except ValueError:\n",
        "      return None\n",
        "\n",
        "test_samples = generate_tests(num_examples=100, seed=212)"
      ],
      "metadata": {
        "id": "k_2LWaCM7ib1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup Trajectories\n",
        "from tuned_lens.plotting import PredictionTrajectory\n",
        "\n",
        "line = slice(14, None)\n",
        "\n",
        "def gen_tra(model, tokenizer, input_ids, targets):\n",
        "  trajectory = PredictionTrajectory.from_lens_and_model(\n",
        "      tuned_lens,\n",
        "      model=model,\n",
        "      tokenizer=tokenizer,\n",
        "      input_ids=input_ids,\n",
        "      targets=targets,\n",
        "  ).slice_sequence(line)\n",
        "  # print(trajectory.max_probability().trajectory_labels.hover_over_entries)\n",
        "                            # shape: 13*13*10*2 = n_layers x sequence_length x n_hover_over_entries x cols\n",
        "  return trajectory\n",
        "\n",
        "def calculate_label_probs(model, test_samples):\n",
        "  probs = np.zeros((13, 13, 100))\n",
        "  for i in range(100):\n",
        "    curr_sample = test_samples[i]\n",
        "    input = tokenizer.encode(curr_sample)\n",
        "    # input = torch.tensor(input, device=device)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input)[14:]\n",
        "    tokens = [token.replace(\"Ġ\", \"_\") for token in tokens]\n",
        "    # print(tokens)\n",
        "    target = input[1:] + [tokenizer.eos_token_id]\n",
        "    curr_trajectory = gen_tra(model, tokenizer, input, target)\n",
        "    probabilities = curr_trajectory.max_probability().trajectory_labels.hover_over_entries\n",
        "    for j in range(13):\n",
        "      for k in range(13):\n",
        "        label_prob = {probabilities[j][k][l][0]: probabilities[j][k][l][1] for l in range(10)}\n",
        "        if k == 12:\n",
        "          true_label = '<|endo…'\n",
        "          # print([probabilities[j][k][l][0] for l in range(10)][0])\n",
        "        else:\n",
        "          true_label = tokens[k+1]\n",
        "        true_prob = 0\n",
        "        if true_label in label_prob.keys():\n",
        "          true_prob = label_prob[true_label]\n",
        "          true_prob = percentage_to_float(true_prob)\n",
        "        # print(true_label, true_prob)\n",
        "        probs[j][k][i] = true_prob\n",
        "        # print(label_prob)\n",
        "\n",
        "  probs = np.mean(probs, axis=2)\n",
        "  probs = probs.tolist()\n",
        "  return probs\n",
        "\n",
        "# trajectory_1 = gen_tra(model, tokenizer, input_1, targets_1)\n",
        "# trajectory_2 = gen_tra(model, tokenizer, input_2, targets_2)\n",
        "# trajectory_3 = gen_tra(model, tokenizer, input_3, targets_3)\n",
        "# trajectory_4 = gen_tra(model, tokenizer, input_4, targets_4)\n",
        "# trajectory_5 = gen_tra(model, tokenizer, input_5, targets_5)\n",
        "# trajectory_6 = gen_tra(model, tokenizer, input_6, targets_6)"
      ],
      "metadata": {
        "id": "ofV_iTY27rUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_true_label_prob = calculate_label_probs(model, test_samples)\n",
        "print(len(avg_true_label_prob))"
      ],
      "metadata": {
        "id": "8l3Hw7ob8Gpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(avg_true_label_prob)\n",
        "plt.gca().invert_yaxis()"
      ],
      "metadata": {
        "id": "abCImRY28Kf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neuron Ablation"
      ],
      "metadata": {
        "id": "gb72Cpy58L3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Neuron Ablation\n",
        "\n",
        "import copy\n",
        "\n",
        "# def ablate_head(module, inp, out):\n",
        "#     # out is a tuple, the first element is the attention output\n",
        "#     out = list(out)  # Convert the tuple to a list to allow modification\n",
        "#     out[0] = out[0].clone()  # Clone the first element (attention output)\n",
        "#     out[0][..., start:end] = 0  # Apply ablation to the cloned tensor\n",
        "#     return tuple(out)  # Convert the list back to a tuple\n",
        "\n",
        "n_head = model.config.n_head                # 12 for gpt2-small\n",
        "head_size = model.config.hidden_size // n_head\n",
        "\n",
        "def calculate_ablated_prob(layer):\n",
        "  ablated_prob_dict = {}\n",
        "\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "  original_weights = model.transformer.h[layer].attn.c_attn.weight.data.clone() # Store original weights\n",
        "\n",
        "  for h in range(n_head):\n",
        "    start, end = h * head_size, (h + 1) * head_size\n",
        "\n",
        "    # copy the model to model_ablated\n",
        "    model_ablated = copy.deepcopy(model)\n",
        "\n",
        "    # manually set the outputs the neurons of that head to be zero\n",
        "    weights = model_ablated.transformer.h[layer].attn.c_attn.weight\n",
        "\n",
        "    # Create a new tensor with the same data and requires_grad=False\n",
        "    # This is a leaf variable, so we can change requires_grad\n",
        "    new_weights = torch.tensor(weights.data, requires_grad=False, device=weights.device)\n",
        "\n",
        "    # Apply ablation to the new_weights tensor\n",
        "    new_weights[:, start:end] = 0\n",
        "\n",
        "    # Assign the modified weights back to the model\n",
        "    model_ablated.transformer.h[layer].attn.c_attn.weight = torch.nn.Parameter(new_weights)\n",
        "    # print(h, \"th head:\")\n",
        "    # print(model_ablated.transformer.h[layer_i].attn.c_attn.weight)\n",
        "\n",
        "    avg_true_label_prob_ablated = calculate_label_probs(model_ablated, test_samples)\n",
        "    ablated_prob_dict[h] = avg_true_label_prob_ablated\n",
        "\n",
        "  return ablated_prob_dict\n"
      ],
      "metadata": {
        "id": "lEQUblo-B1Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_to_ablated_probs = {}\n",
        "for i in range(12): # not 13\n",
        "  print(\"Calculating at the\", i, \"th layer:\")\n",
        "  ablated_prob_dict = calculate_ablated_prob(i)\n",
        "  layer_to_ablated_probs[i] = ablated_prob_dict\n",
        "  # store the results\n",
        "  with open(\"../ablation_results.json\", \"w\") as f:\n",
        "    json.dump(layer_to_ablated_probs, f, indent=4)"
      ],
      "metadata": {
        "id": "Q1uQ4NpCC0kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_to_ablated_probs[\"no ablation\"] = avg_true_label_prob\n",
        "with open(\"../ablation_results.json\", \"w\") as f:\n",
        "  json.dump(layer_to_ablated_probs, f, indent=4)"
      ],
      "metadata": {
        "id": "7VNIS4CqC6mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-running Results\n",
        "If you don't want to run your self, load our results!"
      ],
      "metadata": {
        "id": "0WUhBR1yDGh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"ablation_results.json\", \"r\") as f:\n",
        "  data = json.load(f)"
      ],
      "metadata": {
        "id": "r3-iKldsDGSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot Probability Changes (Each Graph Has a Different Scale)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from matplotlib.colors import PowerNorm\n",
        "# from matplotlib.colors import DivergingNorm\n",
        "layer_to_ablated_probs = data\n",
        "x_labels = [f\"Token{i}\" for i in range(13)]\n",
        "y_labels = [f\"Layer{i}\" for i in range(12)] + [\"Output\"]\n",
        "\n",
        "for i in range(12): # not 13\n",
        "  fig, axes = plt.subplots(4, 3, figsize=(15, 12))\n",
        "  fig.suptitle(f\"Changes in Next Correct Token Prediction Probilities after Head Ablation at Layer {i}\", fontsize=16)\n",
        "  cbar_ax = fig.add_axes([0.92, 0.3, 0.02, 0.4])\n",
        "  plt.tight_layout(rect=[0, 0, 1, 1])\n",
        "  vmin = 100\n",
        "  vmax = -100\n",
        "  diff = []\n",
        "  for j in range(12): # num of heads\n",
        "    diff.append(np.array(layer_to_ablated_probs[str(i)][str(j)]) - np.array(layer_to_ablated_probs[\"no ablation\"]))\n",
        "    vmin = np.minimum(np.min(diff[j]), vmin)\n",
        "    vmax = np.maximum(np.max(diff[j]), vmax)\n",
        "    # print(vmin, vmax)\n",
        "\n",
        "\n",
        "  for j in range(12): # num of heads\n",
        "    plt_i, plt_j = j // 3, j % 3\n",
        "    ax = axes[plt_i, plt_j]\n",
        "    # sns.heatmap(diff[j], ax=ax, vmin=-0.35, vmax=0.1, cbar=(j == 0), cbar_ax=cbar_ax if j == 0 else None, cmap=\"coolwarm\")\n",
        "    # sns.heatmap(diff[j], ax=ax, cmap=\"coolwarm\", norm=PowerNorm(gamma=0.5, vmin=vmin, vmax=vmax), center=0, cbar=(j == 0), cbar_ax=cbar_ax if j == 0 else None)\n",
        "    sns.heatmap(\n",
        "    diff[j],\n",
        "    ax=ax,\n",
        "    cmap=\"coolwarm\",\n",
        "    vmin=vmin,\n",
        "    vmax=vmax,\n",
        "    center=0,          # ← this makes 0.0 map to the center color (white)\n",
        "    cbar=(j == 0),\n",
        "    cbar_ax=(cbar_ax if j == 0 else None),\n",
        "    )\n",
        "\n",
        "    # Add units to the colorbar\n",
        "    if j == 0:\n",
        "        cbar = ax.collections[0].colorbar\n",
        "        cbar.set_label(\"Change in Probability\")  # Set the colorbar label\n",
        "\n",
        "\n",
        "    ax.set_title(f\"Head {j} Ablated\")\n",
        "    ax.set_xticks(np.arange(len(x_labels)) + 0.5)\n",
        "    ax.set_xticklabels(x_labels, rotation=45, ha=\"right\", fontsize=8)\n",
        "\n",
        "    ax.set_yticks(np.arange(len(y_labels)) + 0.5)\n",
        "    ax.set_yticklabels(y_labels, rotation=0, fontsize=8)\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "  plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "0hCelxLBDTU5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwOIprWBrD9+w6y4WfVn8b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claire-fang/cs182-final-project/blob/main/mod)subtraction/GPT2_mod_subtraction_toy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9wVOJ-WJPws"
      },
      "outputs": [],
      "source": [
        "# @title Mount your Google Drive\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "DRIVE_PATH = '/content/gdrive/MyDrive/gpt2_toy'\n",
        "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
        "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "  %mkdir $DRIVE_PATH\n",
        "\n",
        "%cd $DRIVE_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 Mod Subtraction Fine-tuning\n",
        "This notebook fine-tunes GPT-2 on a custom mod-sub dataset, masks the prompt tokens in loss so the model only learns to predict the output list, and supports inference."
      ],
      "metadata": {
        "id": "NjA9iH7VJZGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    GPT2Tokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")"
      ],
      "metadata": {
        "id": "-1mEeu1gJXij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModSubDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, max_length=128):\n",
        "        with open(file_path, 'r') as f:\n",
        "            text = f.read().strip()\n",
        "        self.examples = text.split('\\n\\n')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        raw = self.examples[idx]\n",
        "        prompt, answer = raw.split('\\n', 1)\n",
        "        eos = self.tokenizer.eos_token\n",
        "\n",
        "        # Inject EOS between prompt and answer, and at the end\n",
        "        full = prompt + eos + answer + eos\n",
        "        tok = self.tokenizer(\n",
        "            full,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': tok['input_ids'].squeeze(0),\n",
        "            'attention_mask': tok['attention_mask'].squeeze(0)\n",
        "        }"
      ],
      "metadata": {
        "id": "KuhgLx51JbRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def modsub_collate(batch):\n",
        "    # 1) pull out lists of individual tensors\n",
        "    input_ids_list     = [item['input_ids']     for item in batch]\n",
        "    attention_mask_list= [item['attention_mask'] for item in batch]\n",
        "\n",
        "    # 2) pad up to the longest in this batch\n",
        "    input_ids     = pad_sequence(input_ids_list,     batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_mask= pad_sequence(attention_mask_list,batch_first=True, padding_value=0)\n",
        "\n",
        "    # 3) clone for labels\n",
        "    labels = input_ids.clone()\n",
        "\n",
        "    # 4) mask out the prompt (everything up to and including the first EOS)\n",
        "    eos_id = tokenizer.eos_token_id\n",
        "    for i, seq in enumerate(input_ids):\n",
        "        eos_positions = (seq == eos_id).nonzero(as_tuple=True)[0]\n",
        "        if len(eos_positions)>0:\n",
        "            prompt_end = eos_positions[0].item() + 1\n",
        "        else:\n",
        "            prompt_end = 0\n",
        "        labels[i, :prompt_end] = -100\n",
        "\n",
        "    # 5) mask out any padding positions\n",
        "    labels[attention_mask == 0] = -100\n",
        "\n",
        "    return {\n",
        "        'input_ids':      input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'labels':         labels\n",
        "    }"
      ],
      "metadata": {
        "id": "p798F5WIJcuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_modular_subtraction_examples(num_examples=3000, mod=97, seed=123):\n",
        "    random.seed(seed)\n",
        "    examples = []\n",
        "    for _ in range(num_examples):\n",
        "        a = random.randint(0, 100)\n",
        "        b = random.randint(0, 100)\n",
        "        result = (a - b) % mod\n",
        "        examples.append(f\"Input: {a} - {b} mod {mod}\\nOutput: {result}\")\n",
        "    return examples\n",
        "\n",
        "with open(\"mod_sub_train.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\\n\".join(generate_modular_subtraction_examples(seed=123)))\n",
        "\n",
        "# with open(\"mod_sub_val.txt\", \"w\") as f:\n",
        "#     f.write(\"\\n\\n\".join(generate_modular_subtraction_examples(num_examples=2000, seed=124)))\n"
      ],
      "metadata": {
        "id": "tp-emQk_JeBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_file, model_name='gpt2', output_dir='/content/gdrive/MyDrive/Outputs/gpt2_output',\n",
        "          batch_size=32, epochs=50, max_length=128, save_steps=500):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Tokenizer\n",
        "    global tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # Save tokenizer for inference BEFORE training\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    # dataset\n",
        "    ds = ModSubDataset(train_file, tokenizer, max_length)\n",
        "    # model\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    model.config.eos_token_id = tokenizer.eos_token_id\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    # training args\n",
        "    args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        overwrite_output_dir=True,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        num_train_epochs=epochs,\n",
        "        save_steps=save_steps,\n",
        "        logging_steps=100,\n",
        "        report_to='none'\n",
        "    )\n",
        "    # trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        data_collator=modsub_collate,\n",
        "        train_dataset=ds\n",
        "    )\n",
        "    trainer.train()\n",
        "    trainer.save_model()\n"
      ],
      "metadata": {
        "id": "mVGtQ-6lJfft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Training"
      ],
      "metadata": {
        "id": "YEWTheRaJg-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train('/content/gdrive/MyDrive/Datasets/sort_sub_train_2.txt')"
      ],
      "metadata": {
        "id": "2NW8R6dIJijs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Inference"
      ],
      "metadata": {
        "id": "Uf1Qo6NbJl1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(input_sequence: str,\n",
        "          model_dir: str = '/content/gdrive/MyDrive/Outputs/gpt2_output', # gpt2_output3 is the best result\n",
        "          max_new_tokens: int = 20):\n",
        "    # 1) Load tokenizer & model\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.add_special_tokens({'pad_token':'<|pad|>'})\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    model.config.eos_token_id = tokenizer.eos_token_id\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device).eval()\n",
        "\n",
        "    # 2) Tokenize with attention mask\n",
        "    enc = tokenizer(\n",
        "        input_sequence,\n",
        "        return_tensors='pt',\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        add_special_tokens=False\n",
        "    )\n",
        "    input_ids = enc['input_ids'].to(device)\n",
        "    attention_mask = enc['attention_mask'].to(device)\n",
        "\n",
        "    # 3) Generate *only* max_new_tokens\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,    # ← switch to greedy\n",
        "        num_beams=1,        # ← no beam search, just greedy\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "    # 4) Slice off the prompt tokens and decode only the new ones\n",
        "    gen_tokens = outputs[0, input_ids.shape[-1]:]\n",
        "    result = tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
        "    result = result.lstrip()\n",
        "    # print(result)\n",
        "    return result\n",
        "\n"
      ],
      "metadata": {
        "id": "cJdHnueHJm-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(infer(\"Input: 6 - 34 mod 97\\nOutput:\"))\n",
        "print(infer(\"Input: 6 - 35 mod 97\\nOutput:\"))\n",
        "print(infer(\"Input: 6 - 36 mod 97\\nOutput:\"))\n",
        "print(infer(\"Input: 6 - 37 mod 97\\nOutput:\"))\n",
        "print(infer(\"Input: 6 - 38 mod 97\\nOutput:\"))\n",
        "print(infer(\"Input: 6 - 39 mod 97\\nOutput:\"))\n",
        "print(infer(\"Input: 6 - 40 mod 97\\nOutput:\"))"
      ],
      "metadata": {
        "id": "9KO-5bo7Jn-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(infer(\"Input: 101 - 34 mod 97\\nOutput:\"))\n",
        "print(infer(\"Input: 102 - 34 mod 97\\nOutput:\"))\n",
        "print(infer(\"Input: 103 - 34 mod 97\\nOutput:\"))\n",
        "print(infer(\"Input: 104 - 34 mod 97\\nOutput:\"))\n",
        "print(infer(\"Input: 105 - 34 mod 97\\nOutput:\"))"
      ],
      "metadata": {
        "id": "1CP3Ku4SJo9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test Accuracy"
      ],
      "metadata": {
        "id": "jTP40h0zJsTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "num_correct = 0\n",
        "num_total = 1000\n",
        "\n",
        "for i in range(num_total):\n",
        "    a = random.randint(0, 100)\n",
        "    b = random.randint(0, 100)\n",
        "    mod = 97\n",
        "\n",
        "    prompt = f\"Input: {a} - {b} mod {mod}\\nOutput:\"\n",
        "    expected = (a - b) % mod\n",
        "\n",
        "    predicted = infer(prompt, model_dir='/content/gdrive/MyDrive/Outputs/gpt2_output')\n",
        "\n",
        "    # Try to extract an integer from model output\n",
        "    try:\n",
        "        predicted_int = int(predicted.strip().split()[0])\n",
        "    except:\n",
        "        predicted_int = None\n",
        "\n",
        "    if predicted_int == expected:\n",
        "        num_correct += 1\n",
        "\n",
        "    # if i % 100 == 0:\n",
        "        # print(f\"Step {i}: Current Accuracy = {num_correct / (i + 1):.3f}\")\n",
        "\n",
        "print(f\"Total tested: {num_total}\")\n",
        "print(f\"Final Accuracy: {num_correct / num_total:.3f}\")"
      ],
      "metadata": {
        "id": "5oOw7v9MJtjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tolerent Test Accuracy"
      ],
      "metadata": {
        "id": "1vJRjDZOJxou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_total):\n",
        "    a = random.randint(0, 100)\n",
        "    b = random.randint(0, 100)\n",
        "    mod = 97\n",
        "\n",
        "    prompt = f\"Input: {a} - {b} mod {mod}\\nOutput:\"\n",
        "    expected = (a - b) % mod\n",
        "\n",
        "    predicted = infer(prompt, model_dir='/content/gdrive/MyDrive/Outputs/gpt2_output')\n",
        "\n",
        "    # Try to extract an integer from model output\n",
        "    try:\n",
        "        predicted_int = int(predicted.strip().split()[0])\n",
        "    except:\n",
        "        predicted_int = None\n",
        "\n",
        "    if abs(predicted_int - expected) <= 3:\n",
        "        num_correct += 1\n",
        "\n",
        "print(f\"Total tested: {num_total}\")\n",
        "print(f\"Final Tolerent Accuracy: {num_correct / num_total:.3f}\")"
      ],
      "metadata": {
        "id": "Ijlse6W-Jyep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis"
      ],
      "metadata": {
        "id": "tfJ3-HrAJ0nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "TMutz8HZJ1bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA"
      ],
      "metadata": {
        "id": "3SFWR4qhJ25o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/gdrive/MyDrive/Outputs/gpt2_output/checkpoint-500\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"/content/gdrive/MyDrive/Outputs/gpt2_output\")\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path).cuda()\n",
        "# model = GPT2LMHeadModel.from_pretrained(model_path).cpu()\n",
        "model.eval()\n",
        "\n",
        "p = 97\n",
        "inputs = []\n",
        "labels = []\n",
        "for a in range(101):\n",
        "    for b in range(101):\n",
        "        prompt = f\"Input: {a} - {b} mod {p}\\nOutput:\"\n",
        "        inputs.append(prompt)\n",
        "        labels.append((a, b, (a - b) % p))\n",
        "\n",
        "# Extract the residual state of the last token\n",
        "residuals = []\n",
        "results = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for prompt, (a, b, result) in tqdm(zip(inputs, labels), total=len(inputs)):\n",
        "        tokens = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "        outputs = model(**tokens, output_hidden_states=True)\n",
        "        last_hidden = outputs.hidden_states[-1][0, -1, :].cpu().numpy()\n",
        "        residuals.append(last_hidden)\n",
        "        results.append(result)\n",
        "\n",
        "# PCA\n",
        "from sklearn.decomposition import PCA\n",
        "residuals = torch.tensor(residuals)\n",
        "projected = PCA(n_components=2).fit_transform(residuals)\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(projected[:, 0], projected[:, 1], c=results, cmap='viridis', s=10)\n",
        "plt.colorbar(scatter, label=\"(a - b) mod 97\")\n",
        "plt.title(\"PCA of hidden states (GPT-2 @ checkpoint-500) for training prompts\")\n",
        "plt.xlabel(\"PC 1\")\n",
        "plt.ylabel(\"PC 2\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kg-qChIJJ4ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "residuals_np = residuals.numpy()\n",
        "residuals_np = StandardScaler().fit_transform(residuals_np)\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)\n",
        "projected_tsne = tsne.fit_transform(residuals_np)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(projected_tsne[:, 0], projected_tsne[:, 1], c=results, cmap='viridis', s=10)\n",
        "plt.colorbar(scatter, label=\"(a - b) mod 97\")\n",
        "plt.title(\"t-SNE of hidden states (GPT-2 @ checkpoint-500)\")\n",
        "plt.xlabel(\"t-SNE dim 1\")\n",
        "plt.ylabel(\"t-SNE dim 2\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SnRI_GhWJ6L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/gdrive/MyDrive/Outputs/gpt2_output/checkpoint-2000\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"/content/gdrive/MyDrive/Outputs/gpt2_output\")\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path).cuda()\n",
        "# model = GPT2LMHeadModel.from_pretrained(model_path).cpu()\n",
        "model.eval()\n",
        "\n",
        "p = 97\n",
        "inputs = []\n",
        "labels = []\n",
        "for a in range(101):\n",
        "    for b in range(101):\n",
        "        prompt = f\"Input: {a} - {b} mod {p}\\nOutput:\"\n",
        "        inputs.append(prompt)\n",
        "        labels.append((a, b, (a - b) % p))\n",
        "\n",
        "# Extract the residual state of the last token\n",
        "residuals = []\n",
        "results = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for prompt, (a, b, result) in tqdm(zip(inputs, labels), total=len(inputs)):\n",
        "        tokens = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "        outputs = model(**tokens, output_hidden_states=True)\n",
        "        last_hidden = outputs.hidden_states[-1][0, -1, :].cpu().numpy()\n",
        "        residuals.append(last_hidden)\n",
        "        results.append(result)\n",
        "\n",
        "# PCA\n",
        "from sklearn.decomposition import PCA\n",
        "residuals = torch.tensor(residuals)\n",
        "projected = PCA(n_components=2).fit_transform(residuals)\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(projected[:, 0], projected[:, 1], c=results, cmap='viridis', s=10)\n",
        "plt.colorbar(scatter, label=\"(a - b) mod 97\")\n",
        "plt.title(\"PCA of hidden states (GPT-2 @ checkpoint-2000) for training prompts\")\n",
        "plt.xlabel(\"PC 1\")\n",
        "plt.ylabel(\"PC 2\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cr6j84jZJ7dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "residuals_np = residuals.numpy()\n",
        "residuals_np = StandardScaler().fit_transform(residuals_np)\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)\n",
        "projected_tsne = tsne.fit_transform(residuals_np)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(projected_tsne[:, 0], projected_tsne[:, 1], c=results, cmap='viridis', s=10)\n",
        "plt.colorbar(scatter, label=\"(a - b) mod 97\")\n",
        "plt.title(\"t-SNE of hidden states (GPT-2 @ checkpoint-2000)\")\n",
        "plt.xlabel(\"t-SNE dim 1\")\n",
        "plt.ylabel(\"t-SNE dim 2\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VYdg47PRJ8jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/gdrive/MyDrive/Outputs/gpt2_output\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"/content/gdrive/MyDrive/Outputs/gpt2_output\")\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path).cuda()\n",
        "# model = GPT2LMHeadModel.from_pretrained(model_path).cpu()\n",
        "model.eval()\n",
        "\n",
        "p = 97\n",
        "inputs = []\n",
        "labels = []\n",
        "for a in range(101):\n",
        "    for b in range(101):\n",
        "        prompt = f\"Input: {a} - {b} mod {p}\\nOutput:\"\n",
        "        inputs.append(prompt)\n",
        "        labels.append((a, b, (a - b) % p))\n",
        "\n",
        "# Extract the residual state of the last token\n",
        "residuals = []\n",
        "results = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for prompt, (a, b, result) in tqdm(zip(inputs, labels), total=len(inputs)):\n",
        "        tokens = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "        outputs = model(**tokens, output_hidden_states=True)\n",
        "        last_hidden = outputs.hidden_states[-1][0, -1, :].cpu().numpy()\n",
        "        residuals.append(last_hidden)\n",
        "        results.append(result)\n",
        "\n",
        "# PCA\n",
        "from sklearn.decomposition import PCA\n",
        "residuals = torch.tensor(residuals)\n",
        "projected = PCA(n_components=2).fit_transform(residuals)\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(projected[:, 0], projected[:, 1], c=results, cmap='viridis', s=10)\n",
        "plt.colorbar(scatter, label=\"(a - b) mod 97\")\n",
        "plt.title(\"PCA of hidden states (GPT-2 @ final) for training prompts\")\n",
        "plt.xlabel(\"PC 1\")\n",
        "plt.ylabel(\"PC 2\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yR5RW33uJ9oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "residuals_np = residuals.numpy()\n",
        "residuals_np = StandardScaler().fit_transform(residuals_np)\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)\n",
        "projected_tsne = tsne.fit_transform(residuals_np)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(projected_tsne[:, 0], projected_tsne[:, 1], c=results, cmap='viridis', s=10)\n",
        "plt.colorbar(scatter, label=\"(a - b) mod 97\")\n",
        "plt.title(\"t-SNE of hidden states (GPT-2 @ final)\")\n",
        "plt.xlabel(\"t-SNE dim 1\")\n",
        "plt.ylabel(\"t-SNE dim 2\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JV7wSwvlKBA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Linear Probing"
      ],
      "metadata": {
        "id": "jM39GrZCKCWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "model_path = '/content/gdrive/MyDrive/Outputs/gpt2_output'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2Model.from_pretrained(model_path, output_hidden_states=True).to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "SMg_lY_JKDQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def probe_all_layers(model, tokenizer, prompts, targets, max_length=64):\n",
        "    accuracies = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for layer in range(13):  # embeddings + 12 transformer layers\n",
        "            features = []\n",
        "            for p in prompts:\n",
        "                tokens = tokenizer(p, return_tensors=\"pt\", truncation=True, max_length=max_length).to(device)\n",
        "                outputs = model(**tokens)\n",
        "                h = outputs.hidden_states[layer][0, -1].cpu().numpy()  # last token\n",
        "                features.append(h)\n",
        "            clf = LogisticRegression(max_iter=1000).fit(features, targets)\n",
        "            acc = clf.score(features, targets)\n",
        "            print(f\"Layer {layer:2d} probe accuracy: {acc:.4f}\")\n",
        "            accuracies.append(acc)\n",
        "    return accuracies\n",
        "\n",
        "num_samples = 500\n",
        "a_vals = np.random.randint(0, 100, num_samples)\n",
        "b_vals = np.random.randint(0, 100, num_samples)\n",
        "prompts = [f\"Input: {a} - {b} mod 97\\nOutput:\" for a, b in zip(a_vals, b_vals)]\n",
        "labels = [(a - b) % 97 for a, b in zip(a_vals, b_vals)]\n",
        "\n",
        "all_accs = probe_all_layers(model, tokenizer, prompts, labels)"
      ],
      "metadata": {
        "id": "_qeWpo_UKEE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(range(13), all_accs, marker='o', linestyle='-')\n",
        "plt.title(\"Linear Probe Accuracy Across GPT-2 Layers\")\n",
        "plt.xlabel(\"Layer\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.gca().yaxis.set_major_formatter(ticker.FormatStrFormatter('%.4f'))\n",
        "plt.xticks(range(0, 13, 2))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BU0fkYvzKFbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logit Lens"
      ],
      "metadata": {
        "id": "itPp1MOiKGwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "model_path = '/content/gdrive/MyDrive/Outputs/gpt2_output3'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path, output_hidden_states=True).to(device)\n",
        "model.eval()\n",
        "\n",
        "allowed_tokens = [tokenizer.encode(str(i), add_special_tokens=False)[0] for i in range(97)]\n",
        "newline_token = tokenizer.encode(\"\\n\", add_special_tokens=False)[0]\n",
        "allowed_tokens.append(newline_token)\n",
        "\n",
        "def top_digit_from_logits(logits):\n",
        "    mask = torch.full_like(logits, -1e9)\n",
        "    mask[:, allowed_tokens] = logits[:, allowed_tokens]\n",
        "    top_id = int(torch.argmax(mask, dim=-1))\n",
        "    return tokenizer.decode([top_id]).strip() if top_id != newline_token else \"?\"\n",
        "\n",
        "def logit_lens(prompt, layer_to_show=range(12)):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        hidden_states = outputs.hidden_states  # tuple of hidden states per layer\n",
        "\n",
        "    try:\n",
        "        result = eval(prompt.split('Input:')[1].split('mod')[0]) % 97\n",
        "    except:\n",
        "        result = \"?\"\n",
        "\n",
        "    print(f\"Prompt: {prompt}\", end=\" \")\n",
        "    print(infer(prompt))\n",
        "    print(f\"Expected: {result}\\n\")\n",
        "    for layer in layer_to_show:\n",
        "        h = hidden_states[layer][:, -1, :]\n",
        "        logits = model.lm_head(h)\n",
        "        pred = top_digit_from_logits(logits)\n",
        "        tag  = \"✅\" if pred.isdigit() and int(pred) == result else \"❌\"\n",
        "        print(f\"Layer {layer:2d}: {pred:>3} {tag}\")\n",
        "\n",
        "# Example\n",
        "a, b = 88, 34\n",
        "prompt = f\"Input: {a} - {b} mod 97\\nOutput:\"\n",
        "logit_lens(prompt)"
      ],
      "metadata": {
        "id": "g2KIIw7XKHfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────────  Layer‑12 Logit‑Lens Accuracy  ─────────────\n",
        "# Allowed tokens are digits 0‑96 + newline token\n",
        "allowed_tokens = [tokenizer.encode(str(i), add_special_tokens=False)[0] for i in range(97)]\n",
        "newline_tok = tokenizer.encode(\"\\n\", add_special_tokens=False)[0]\n",
        "allowed_tokens.append(newline_tok)\n",
        "\n",
        "def top_digit_from_logits(logits):\n",
        "    \"\"\"Return the top token restricted to digits 0‑96 (+ newline).\"\"\"\n",
        "    mask = torch.full_like(logits, -1e9)\n",
        "    mask[:, allowed_tokens] = logits[:, allowed_tokens]\n",
        "    top_id = int(torch.argmax(mask, dim=-1))\n",
        "    return tokenizer.decode([top_id]).strip() if top_id != newline_tok else \"?\"\n",
        "\n",
        "def layer12_accuracy(num_samples=500):\n",
        "    correct = 0\n",
        "    for _ in range(num_samples):\n",
        "        a, b = random.randint(0, 96), random.randint(0, 96)\n",
        "        prompt = f\"Input: {a} - {b} mod 97\\nOutput:\"\n",
        "        expected = (a - b) % 97\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "            # Get the hidden states from layer 12 (last token's state)\n",
        "            h12 = model(**inputs, output_hidden_states=True).hidden_states[12][0, -1]  # Layer 12, last token\n",
        "            pred_tok = top_digit_from_logits(model.lm_head(h12.unsqueeze(0)))  # Apply lm_head for logits prediction\n",
        "\n",
        "        if pred_tok.isdigit() and int(pred_tok) == expected:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / num_samples\n",
        "\n",
        "# Evaluate on 500 random (a,b) pairs\n",
        "acc = layer12_accuracy(500)\n",
        "print(f\"Layer12 logit‑lens accuracy: {acc:.3f}\")"
      ],
      "metadata": {
        "id": "_UCAeFNUKInj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fourier Analysis"
      ],
      "metadata": {
        "id": "vLD1V6nkKKAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Fourier analysis on the trained model\n",
        "\n",
        "model_path = '/content/gdrive/MyDrive/Outputs/gpt2_output'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path, output_hidden_states=True).to(device)\n",
        "model.eval()\n",
        "\n",
        "def get_final_vec(model, tokenizer, prompt, layer=10):\n",
        "    \"\"\"\n",
        "    Given a prompt and a model, returns the hidden state of the final token\n",
        "    at the specified layer.\n",
        "    \"\"\"\n",
        "    tok = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tok, output_hidden_states=True)\n",
        "        h = outputs.hidden_states[layer]  # [batch, seq_len, hidden_dim]\n",
        "    return h[0, -1].cpu()  # 最后一个 token 的表示\n",
        "\n",
        "def fft_energy_ratio(vec, k=5):\n",
        "    \"\"\"\n",
        "    Compute the ratio of the sum of top-k FFT magnitudes to the total magnitude.\n",
        "    A measure of how much energy is concentrated in the top frequencies.\n",
        "    \"\"\"\n",
        "    coeff = torch.fft.fft(vec.double())\n",
        "    mag = torch.abs(coeff)\n",
        "    return (mag.topk(k).values.sum() / mag.sum()).item()\n",
        "\n",
        "def quiver_plot(vec, title):\n",
        "    \"\"\"\n",
        "    Plot the complex FFT coefficients as arrows (quiver plot).\n",
        "    \"\"\"\n",
        "    coeff = torch.fft.fft(vec.double()).cpu().numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.quiver(\n",
        "        np.zeros_like(coeff.real),\n",
        "        np.zeros_like(coeff.imag),\n",
        "        coeff.real,\n",
        "        coeff.imag,\n",
        "        angles='xy', scale_units='xy', scale=1\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Example prompt (you can change the numbers here)\n",
        "prompt = \"Input: 7 - 4 mod 97\\nOutput:\"\n",
        "\n",
        "# 8a) Extract the hidden vector from layer 10\n",
        "vec_final = get_final_vec(model, prompt, layer=10)\n",
        "\n",
        "# 8b) Compute and print the top-5 FFT energy ratio\n",
        "energy_ratio_final = fft_energy_ratio(vec_final, k=5)\n",
        "print(f\"Final model top-5 FFT energy ratio (layer 10): {energy_ratio_final:.4f}\")\n",
        "\n",
        "# 8c) Visualize the FFT coefficients as a quiver plot\n",
        "quiver_plot(vec_final, \"Layer 10 FFT (final model)\")"
      ],
      "metadata": {
        "id": "vQlioEL0KKv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare early vs final FFT energy ratios to check for “grokking”–style emergence\n",
        "\n",
        "early_ckpt = \"/content/gdrive/MyDrive/Outputs/gpt2_output/checkpoint-3000\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('/content/gdrive/MyDrive/Outputs/gpt2_output')\n",
        "early_model = GPT2LMHeadModel.from_pretrained(early_ckpt, local_files_only=True, output_hidden_states=True).to(device)\n",
        "early_model.eval()\n",
        "\n",
        "prompt = \"Input: 7 - 4 mod 97\\nOutput:\"\n",
        "\n",
        "vec_early = get_final_vec(early_model, tokenizer, prompt, layer=10)\n",
        "energy_ratio_early = fft_energy_ratio(vec_early, k=5)\n",
        "\n",
        "vec_final = get_final_vec(model, tokenizer, prompt, layer=10)\n",
        "energy_ratio_final = fft_energy_ratio(vec_final, k=5)\n",
        "\n",
        "# Print both for comparison\n",
        "print(f\"Early checkpoint top-5 FFT energy ratio (layer 10): {energy_ratio_early:.4f}\")\n",
        "print(f\"Final model top-5 FFT energy ratio (layer 10): {energy_ratio_final:.4f}\")"
      ],
      "metadata": {
        "id": "08VifouxKMkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute top-5 FFT energy ratios for each layer (0–12)\n",
        "# on both an early checkpoint and the final fine-tuned model.\n",
        "\n",
        "early_ckpt = \"/content/gdrive/MyDrive/Outputs/gpt2_output/checkpoint-3000\"\n",
        "final_ckpt = \"/content/gdrive/MyDrive/Outputs/gpt2_output\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(final_ckpt)\n",
        "\n",
        "early_model = GPT2LMHeadModel.from_pretrained(early_ckpt, local_files_only=True, output_hidden_states=True).to(device)\n",
        "final_model = GPT2LMHeadModel.from_pretrained(final_ckpt, output_hidden_states=True).to(device)\n",
        "\n",
        "early_model.eval()\n",
        "final_model.eval()\n",
        "\n",
        "prompt = \"Input: 7 - 4 mod 97\\nOutput:\"\n",
        "\n",
        "\n",
        "def get_final_vec(model, tokenizer, prompt, layer=10):\n",
        "    tokens = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens, output_hidden_states=True)\n",
        "    return outputs.hidden_states[layer][0, -1].cpu()  # 获取最后 token 的 hidden vector\n",
        "\n",
        "def fft_energy_ratio(vec, k=5):\n",
        "    coeff = torch.fft.fft(vec.double())\n",
        "    mag = torch.abs(coeff)\n",
        "    return (mag.topk(k).values.sum() / mag.sum()).item()\n",
        "\n",
        "layers = list(range(13))  # embeddings + 12层 transformer\n",
        "ratios_early = []\n",
        "ratios_final = []\n",
        "\n",
        "for L in layers:\n",
        "    ve = get_final_vec(early_model, tokenizer, prompt, layer=L)\n",
        "    vf = get_final_vec(final_model, tokenizer, prompt, layer=L)\n",
        "    ratios_early.append(fft_energy_ratio(ve, k=5))\n",
        "    ratios_final.append(fft_energy_ratio(vf, k=5))\n",
        "\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.plot(layers, ratios_early, marker='o', label='early checkpoint')\n",
        "plt.plot(layers, ratios_final, marker='s', label='final model')\n",
        "plt.xlabel('Layer')\n",
        "plt.ylabel('Top-5 FFT Energy Ratio')\n",
        "plt.title('FFT Energy Concentration by Layer')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('fft_energy_ratio_by_layer.png', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rY8kzNnWKNlj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}